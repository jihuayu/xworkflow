# LLM stream basic: streams tokens from a mock OpenAI SSE endpoint.
# File: Expected execution result and output values

status = "completed"

[outputs]
text = "Hello world"
